---
title: "Cy Young Prediction Project"
author: "Jonah Bonesteel"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width = 6.7, fig.height = 4)
```

For my final project, I am interested in dealing with baseball statistics. My idea for the project is to identify key metrics that are most significant in determining whether a pitcher receives points for the Cy Young Award. I also hope to create a regression model that can predict a how many Cy Young points a player will receive based on their current statistics. 

Each league's award is voted on by members of the Baseball Writers' Association of America, with one representative from each team. As of the 2010 season, each voter places a vote for first, second, third, fourth, and fifth place among the pitchers of each league. The formula used to calculate the final scores is a weighted sum of the votes. The pitcher with the highest score in each league wins the award. If two pitchers receive the same number of votes, the award is shared. From 1970 to 2009, writers voted for three pitchers, with the formula of five points for a first-place vote, three for a second-place vote and one for a third-place vote. Prior to 1970, writers only voted for the best pitcher and used a formula of one point per vote.My hope for this project is that I will be able to run my models on current player statistics at the end of each season to predict who will win the Cy Young Award race. (source: Wikipedia)

The data I have is an MLB Cy Young Award data set that goes back to 1956. There are 56 variables including the predictor variable "pointsWon", which contains the number of Cy Young points a player received.

## Part 1: Exploratory Analysis and Data Cleaning

```{r}
library(readr)
cy_data <- read_csv("cy_data.csv")
```

In order to alter the data to the format we would like, we must first know the type of each column.

```{r}
sapply(cy_data, class)
```

From part 1 of the project, we know that the number of votes given out dramatically increased to what it still is today in the year 1970. Let's visualize this again.

```{r}
library(ggplot2)
ggplot(cy_data, aes(yearID)) + geom_histogram(aes(fill = ..count..), binwidth = 9) + ggtitle('Total Votes Given Out per Year 1956 - 2016')
```

Due to the dramatic increase in total Cy Young votes per year in 1970, lets start by eliminating values prior to that year.

```{r}
library(dplyr)
cy_data = filter(cy_data, yearID >= 1970)
```

Lets view the distribution of votes now.

```{r}
library(ggplot2)
ggplot(cy_data, aes(yearID)) + geom_histogram(aes(fill = ..count..), binwidth = 9) + ggtitle('Total Votes Given Out per Year 1970 - 2016')
```

To do some preliminary exploratory analysis, lets create a subset object with only the variables we are interested in.

```{r}
subset <- cy_data[,c("pointsWon","ERA","SO","HR","H","BAOpp","WHIP","BB","W","L","IP",'SV')]
```

Lets take a look at summary statistics and a basic correlation matrix for a few of the numeric variables to see if there are any patterns.

```{r}
summary(subset)
library(corrplot)
matrix<-cor(subset)
head(round(matrix,2))
corrplot(matrix, method="number", title="Correlation Matrix", mar=c(0,0,1,0), number.cex=0.70)
```

In preparation for data cleaning, lets do some more exploratory analysis to see if we can identify potential outliers or transformations.

I have a hunch that the Cy Young criteria for starters is different than for relievers. Lets look at some variables that would differ the most between starters and relievers, such as innings pitched and saves.

```{r}
boxplot(subset$IP, main="Boxplot of Innings Pitched", ylab="Innings Pitched")
ggplot(subset, aes(IP)) + geom_histogram(aes(fill = pointsWon), binwidth = 20) + ggtitle('Innings Pitched versus Vote Count')
ggplot(subset, aes(SV)) + geom_histogram(aes(fill = pointsWon), binwidth = 6) + ggtitle('Saves versus Vote Count')
```

There does appear to be a clear divide among starters and relievers. The plot of innings pitched vs points shows there to be a bimodal distribution, where there is a large cluster of data points around the 75 to 100 innings pitched range as well as one around the 200 to 250 innings pitched range. This would make sense to explain starters vs relievers. Relievers would tend to be in the 75-100 innings pitched range, while starters would tend to be 150 innings pitched and above. Additionally, the plots surrounding the saves variable show us the clear divide between who are starters and who are relievers, as it is extremely rare that a starter records a save. However, there are relievers who do not earn saves.

A solution to this problem would be to use the 'games started' and 'games appeared' columns. A reliever ought to be classified as someone who is making lots of game appearances but having few starting appearances. Lets create a "games started rate" column where it is simply = games started / total games.

```{r}
library(tidyverse)
cy_data <- mutate(cy_data, start_rate = GS/G)
```

Define a reliever as someone with a start rate of lower than 25%.

```{r}
cy_data$position <- ifelse(cy_data$start_rate < 0.25, 'Reliever', 'Starter')
```

Plot count of starters vs relievers in the data set.

```{r}
library(ggplot2)
ggplot(data = cy_data, aes(x = position)) +
    geom_bar() + ggtitle('Count of Relievers and Starters')
```

Because of the differences in each position's stats, I will be splitting up the data set to account for each position and run the analysis separately.

```{r}
starterSubset <- subset(cy_data, position == 'Starter')
relieverSubset <- subset(cy_data, position == 'Reliever')
```

One problem I foresee is with the distribution of Cy Young Votes in our data set. Due to the nature of the voting system, the majority of players in this data set will have a low number of votes. Lets visualize this for both starters and relievers.

```{r}
hist(starterSubset$pointsWon,main="Histogram of Cy Young Votes for Starters", xlab="Cy Young Votes Recieved")
hist(relieverSubset$pointsWon,main="Histogram of Cy Young Votes for Relievers", xlab="Cy Young Votes Recieved")
```

Because of the right-skewness of the response variable in both the subsets, we will need to perform a log transformation on the pointsWon variable.

## Part 2: Model Creation

Lets start with starters. First lets subset the whole data set into only include the variables we are interested in.

```{r}
startersData <- starterSubset[,c('pointsWon','W','L','IP','H','HR','BB','SO','BAOpp','ERA','WHIP')]
```

```{r}
startersData %>% summarise_all(~ sum(is.na(.)))
```

Lets remove the erroneous data value in row 169.

```{r}
startersData <- startersData[-c(169), ]
```

Lets get an idea of the overall distributions of each variable.

```{r}
hist(startersData$W,main="Histogram of W for Starters", xlab="W")
hist(startersData$L,main="Histogram of L for Starters", xlab="L")
hist(startersData$IP,main="Histogram of IP for Starters", xlab="IP")
hist(startersData$H,main="Histogram of H for Starters", xlab="H")
hist(startersData$HR,main="Histogram of HR for Starters", xlab="HR")
hist(startersData$BB,main="Histogram of BB for Starters", xlab="BB")
hist(startersData$SO,main="Histogram of SO for Starters", xlab="SO")
hist(startersData$BAOpp,main="Histogram of BAOpp for Starters", xlab="BAOpp")
hist(startersData$ERA,main="Histogram of ERA for Starters", xlab="ERA")
hist(startersData$WHIP,main="Histogram of WHIP for Starters", xlab="WHIP")
```

Let's log transform the variables who are not normally distributed.

```{r}
logStarterVotes <- log(startersData$pointsWon)
hist(logStarterVotes,main="Histogram of Cy Young Votes for Starters", xlab="Cy Young Votes Recieved")
```

Multicollinearity might be an issue in this analysis. Lets look at the correlation matrix for starters subset.

```{r}
summary(startersData)
library(corrplot)
matrix<-cor(startersData)
head(round(matrix,2))
corrplot(matrix, method="number", title="Correlation Matrix for Starters", mar=c(0,0,1,0),number.cex = 0.70)
```

Lets keep this in mind when creating the final model. For now, lets fit an initial model, run the proper diagnostics to check the assumptions for a linear regression model, and determine if any other transformations are necessary

Declare other variables.

```{r}
startersW <- startersData$W
startersL <- startersData$L
startersIP <- startersData$IP
startersH <- startersData$H
startersHR <- startersData$HR
startersBB <- startersData$BB 
startersSO <- startersData$SO
startersBAOpp <- startersData$BAOpp
startersERA <- startersData$ERA
startersWHIP <- startersData$WHIP
```


Fit an initial model with each variable from the subset.

```{r}
startersFull <- lm(logStarterVotes ~ startersW + startersL + startersIP + startersH + startersHR + 
                     startersBB + startersSO + startersBAOpp + startersERA + startersWHIP)
summary(startersFull)
```

The first assumption we will test is the homogeneity of errors assumption. Plot the residuals against the predicted Cy Young Votes. 

```{r}
plot(startersFull$fitted, startersFull$residuals,main="Residuals vs Fitted Plot for Cy Young Votes", xlab="Fitted Values",ylab="Residuals", pch=23,bg="red",cex=1.5,lwd=1.5)
abline(h=0,col="red")
```

There should be no fanning effect in this graph. Since there is a fanning effect, we should be concerned about the homogeneity of errors assumption.

Lets look at a QQ-plot, boxplot and histogram of the residuals with normal curve.

```{r}
res=startersFull$residuals
hist(res, prob = TRUE, main="Histogram of the Residuals", xlab="Residuals",ylab="Density", col=rainbow(6))
lines(density(res), col = 4, lwd = 2)

boxplot(res, main="Box Plot of the Residuals", ylab="Residuals")

library(car)
qqPlot(startersFull, id.n=5, main='QQ Plot')
```

The plots of the residuals look approximately normal, which is what we are looking for. Lets use the residuals from this model to remove outliers and extreme values and run another model.

Remove outliers:

```{r}
rstandard<-rstandard(startersFull)
nonoutlierdf<-data.frame(logStarterVotes,startersW,startersL,startersIP,startersH,startersHR,startersBB,startersSO,
                         startersBAOpp,startersERA,startersWHIP,rstandard)
nonoutlierdf<-nonoutlierdf[!(nonoutlierdf$rstandard >= 2 | nonoutlierdf$rstandard <= -2), ]
```

Create new model without the outliers.

```{r}
startersNoOutlier <- lm(logStarterVotes ~ startersW + startersL + startersIP + startersH + startersHR + 
                     startersBB + startersSO + startersBAOpp + startersERA + startersWHIP, data = nonoutlierdf)
summary(startersNoOutlier)
```

```{r}
plot(startersNoOutlier$fitted, startersNoOutlier$residuals,main="Residuals vs Fitted Plot for Cy Young Votes", xlab="Fitted Values",ylab="Residuals", pch=23,bg="red",cex=1.5,lwd=1.5)
abline(h=0,col="red")
```

This residuals vs fitted plot looks better after removing outliers.

Lets look at a QQ-plot, boxplot and histogram of the residuals with normal curve.

```{r}
res=startersNoOutlier$residuals
hist(res, prob = TRUE, main="Histogram of the Residuals", xlab="Residuals",ylab="Density", col=rainbow(6))
lines(density(res), col = 4, lwd = 2)

boxplot(res, main="Box Plot of the Residuals", ylab="Residuals")

library(car)
qqPlot(startersNoOutlier, id.n=5, main='QQ Plot')
```

Now, lets make a final model by removing the insignificant variables from the model. We will do this by using the model selection criteria provided from the 'leaps' library.

```{r}
library(leaps)
startersDiagnostic <- regsubsets(logStarterVotes ~ startersW + startersL + startersIP + startersH + startersHR + 
                     startersBB + startersSO + startersBAOpp + startersERA + startersWHIP, data = nonoutlierdf)
reg_summary<-summary(startersDiagnostic)
names(reg_summary)
reg_summary$which
par(mfrow = c(2,2))
plot(reg_summary$rss, xlab = "Number of Variables", ylab = "RSS", type = "l")
rss_min<-which.min(reg_summary$rss)
points(rss_min, reg_summary$rss[rss_min],col="blue",cex = 2, pch = 20)
plot(reg_summary$adjr2, xlab = "Number of Variables", ylab = "Adjusted RSq", type = "l")
adjr2_max<-which.max(reg_summary$adjr2)
points(adjr2_max, reg_summary$adjr2[adjr2_max],col="green",cex = 2, pch = 20)

plot(reg_summary$cp, xlab = "Number of Variables", ylab = "Cp", type = "l")
cp_min = which.min(reg_summary$cp) # 7
points(cp_min, reg_summary$cp[cp_min], col = "red", cex = 2, pch = 20)

plot(reg_summary$bic, xlab = "Number of Variables", ylab = "BIC", type = "l")
bic_min = which.min(reg_summary$bic) # 6
points(bic_min, reg_summary$bic[bic_min], col = "red", cex = 2, pch = 20)

```

Model selection using forward selection.

```{r}
# Both Forward selection
startersForward <- regsubsets(logStarterVotes ~ startersW + startersL + startersIP + startersH + startersHR + 
                     startersBB + startersSO + startersBAOpp + startersERA + startersWHIP, data = nonoutlierdf, nvmax=10, method = 'forward')
summary(startersForward)
```

Based off the plots, the recommended number of variables we should keep in the model is 8. Going off the selection table, we will remove BB and ERA from the final model.

Re- name variables and create final predictive model:

```{r}
logStarterVotes <- nonoutlierdf$logStarterVotes
startersW <- nonoutlierdf$startersW
startersL <- nonoutlierdf$startersL
startersIP <- nonoutlierdf$startersIP
startersH <- nonoutlierdf$startersH
startersHR <- nonoutlierdf$startersHR
startersSO <- nonoutlierdf$startersSO
startersBAOpp <- nonoutlierdf$startersBAOpp
startersWHIP <- nonoutlierdf$startersWHIP

startersFinal <- lm(logStarterVotes ~ startersW + startersL + startersIP + startersH + startersHR + startersSO + startersBAOpp + startersWHIP)
summary(startersFinal)
```

Now, lets create the model for relievers.

```{r}
relieverData <- relieverSubset[,c('pointsWon','W','L','IP','H','HR','BB','SO','BAOpp','ERA','WHIP','SV')]
```

Check if any columns have null values.

```{r}
relieverData %>% summarise_all(~ sum(is.na(.)))
```

Lets get an idea of the overall distributions of each variable.

```{r}
hist(relieverData$W,main="Histogram of W for Relievers", xlab="W")
hist(relieverData$L,main="Histogram of L for Relievers", xlab="L")
hist(relieverData$IP,main="Histogram of IP for Relievers", xlab="IP")
hist(relieverData$H,main="Histogram of H for Relievers", xlab="H")
hist(relieverData$HR,main="Histogram of HR for Relievers", xlab="HR")
hist(relieverData$BB,main="Histogram of BB for Relievers", xlab="BB")
hist(relieverData$SO,main="Histogram of SO for Relievers", xlab="SO")
hist(relieverData$BAOpp,main="Histogram of BAOpp for Relievers", xlab="BAOpp")
hist(relieverData$ERA,main="Histogram of ERA for Relievers", xlab="ERA")
hist(relieverData$WHIP,main="Histogram of WHIP for Relievers", xlab="WHIP")
hist(relieverData$SV,main="Histogram of SV for Relievers", xlab="SV")
```

Let's log transform the response variable.

```{r}
logRelieverVotes <- log(relieverData$pointsWon)
hist(logRelieverVotes,main="Histogram of Cy Young Votes for Relievers", xlab="Cy Young Votes Recieved")
```

Multicollinearity might be an issue in this analysis. Lets look at the correlation matrix for relievers subset.

```{r}
summary(relieverData)
library(corrplot)
matrix<-cor(relieverData)
head(round(matrix,2))
corrplot(matrix, method="number", title="Correlation Matrix for Relievers", mar=c(0,0,1,0),number.cex = 0.70)
```

Lets keep this in mind when creating the final model. For now, lets fit an initial model, run the proper diagnostics to check the assumptions for a linear regression model, and determine if any other transformations are necessary

Declare other variables.

```{r}
relieverW <- relieverData$W
relieverL <- relieverData$L
relieverIP <- relieverData$IP
relieverH <- relieverData$H
relieverHR <- relieverData$HR
relieverBB <- relieverData$BB 
relieverSO <- relieverData$SO
relieverBAOpp <- relieverData$BAOpp
relieverERA <- relieverData$ERA
relieverWHIP <- relieverData$WHIP
relieverSV <- relieverData$SV
```


Fit an initial model with each variable from the subset.

```{r}
relieverFull <- lm(logRelieverVotes ~ relieverW + relieverL + relieverIP + relieverH + relieverHR + 
                     relieverBB + relieverSO + relieverBAOpp + relieverERA + relieverWHIP + relieverSV)
summary(relieverFull)
```

The first assumption we will test is the homogeneity of errors assumption. Plot the residuals against the predicted Cy Young Votes. 

```{r}
plot(relieverFull$fitted, relieverFull$residuals,main="Residuals vs Fitted Plot for Cy Young Votes", xlab="Fitted Values",ylab="Residuals", pch=23,bg="red",cex=1.5,lwd=1.5)
abline(h=0,col="red")
```

The randomness in this plot is what we are looking for when assessing the homogeneity of errors assumption.

Lets look at a QQ-plot, boxplot and histogram of the residuals with normal curve.

```{r}
res=relieverFull$residuals
hist(res, prob = TRUE, main="Histogram of the Residuals", xlab="Residuals",ylab="Density", col=rainbow(6))
lines(density(res), col = 4, lwd = 2)

boxplot(res, main="Box Plot of the Residuals", ylab="Residuals")

library(car)
qqPlot(relieverFull, id.n=5, main='QQ Plot')
```

The plots of the residuals look approximately normal, which is what we are looking for. Lets use the residuals from this model to remove outliers and extreme values and run another model.

Remove outliers:

```{r}
rstandard<-rstandard(relieverFull)
nonoutlierdf<-data.frame(logRelieverVotes,relieverW,relieverL,relieverIP,relieverH,relieverHR,relieverBB,relieverSO,
                         relieverBAOpp,relieverERA,relieverWHIP,relieverSV,rstandard)
nonoutlierdf<-nonoutlierdf[!(nonoutlierdf$rstandard >= 2 | nonoutlierdf$rstandard <= -2), ]
```

Create new model without the outliers.

```{r}
relieversNoOutlier <- lm(logRelieverVotes ~ relieverW + relieverL + relieverIP + relieverH + relieverHR + 
                     relieverBB + relieverSO + relieverBAOpp + relieverERA + relieverWHIP + relieverSV, data = nonoutlierdf)
summary(relieversNoOutlier)
```

Lets look at the residuals vs fitted plot without outliers.

```{r}
plot(relieversNoOutlier$fitted, relieversNoOutlier$residuals,main="Residuals vs Fitted Plot for Cy Young Votes", xlab="Fitted Values",ylab="Residuals", pch=23,bg="red",cex=1.5,lwd=1.5)
abline(h=0,col="red")
```

Lets look at a QQ-plot, boxplot and histogram of the residuals with normal curve.

```{r}
res=relieversNoOutlier$residuals
hist(res, prob = TRUE, main="Histogram of the Residuals", xlab="Residuals",ylab="Density", col=rainbow(6))
lines(density(res), col = 4, lwd = 2)

boxplot(res, main="Box Plot of the Residuals", ylab="Residuals")

library(car)
qqPlot(relieversNoOutlier, id.n=5, main='QQ Plot')
```

Now, lets make a final model by removing the insignificant variables from the model. We will do this by using the model selection criteria provided from the 'leaps' library.

```{r}
library(leaps)
relieverDiagnostic <- regsubsets(logRelieverVotes ~ relieverW + relieverL + relieverIP + relieverH + relieverHR + 
                     relieverBB + relieverSO + relieverBAOpp + relieverERA + relieverWHIP + relieverSV, data = nonoutlierdf)
reg_summary<-summary(startersDiagnostic)
names(reg_summary)
reg_summary$which
par(mfrow = c(2,2))
plot(reg_summary$rss, xlab = "Number of Variables", ylab = "RSS", type = "l")
rss_min<-which.min(reg_summary$rss)
points(rss_min, reg_summary$rss[rss_min],col="blue",cex = 2, pch = 20)
plot(reg_summary$adjr2, xlab = "Number of Variables", ylab = "Adjusted RSq", type = "l")
adjr2_max<-which.max(reg_summary$adjr2)
points(adjr2_max, reg_summary$adjr2[adjr2_max],col="green",cex = 2, pch = 20)

plot(reg_summary$cp, xlab = "Number of Variables", ylab = "Cp", type = "l")
cp_min = which.min(reg_summary$cp) # 7
points(cp_min, reg_summary$cp[cp_min], col = "red", cex = 2, pch = 20)

plot(reg_summary$bic, xlab = "Number of Variables", ylab = "BIC", type = "l")
bic_min = which.min(reg_summary$bic) # 6
points(bic_min, reg_summary$bic[bic_min], col = "red", cex = 2, pch = 20)

```

Model selection using forward selection.

```{r}
# Both Forward selection
relieverForward <- regsubsets(logRelieverVotes ~ relieverW + relieverL + relieverIP + relieverH + relieverHR + 
                     relieverBB + relieverSO + relieverBAOpp + relieverERA + relieverWHIP + relieverSV, data = nonoutlierdf, nvmax=11, method='forward')
summary(relieverForward)
```

Based off the plots, the recommended number of variables we should keep in the model is 8. We will remove H, BAOpp, and WHIP from the final model.

Lets re-declare objects for the variables used in the final model. 

```{r}
logRelieverVotes <- nonoutlierdf$logRelieverVotes
relieverW <- nonoutlierdf$relieverW
relieverL <- nonoutlierdf$relieverL
relieverIP <- nonoutlierdf$relieverIP
relieverBB <- nonoutlierdf$relieverBB
relieverSO <- nonoutlierdf$relieverSO
relieverHR <- nonoutlierdf$relieverHR
relieverERA <- nonoutlierdf$relieverERA
relieverSV <- nonoutlierdf$relieverSV
```

Final predictive model:

```{r}
relieverFinal <- lm(logRelieverVotes ~ relieverW + relieverL + relieverIP + relieverHR + 
                     relieverBB + relieverSO + relieverERA + relieverSV)
summary(relieverFinal)
```

## Part 3: Predictions for 2022 Awards Recipients

Lets test these models on data from the 2022 MLB season. The recipients of the awards are announced on November 7th, 2022. These predictions were made on November 6th.

The data sets I will be importing are of qualified starters and relievers from statcast.com.

```{r}
stats_2022 <- read_csv("stats.csv")
```

This data only contains starting pitchers, so lets run our starters predictive model and make predictions.

```{r}
startersW <- c(stats_2022$W)
startersL <- c(stats_2022$L)
startersIP <- c(stats_2022$IP)
startersH <- c(stats_2022$H)
startersHR <- c(stats_2022$HR)
startersSO <- c(stats_2022$SO)
startersBAOpp <- c(stats_2022$BAOpp)
startersWHIP <- c(stats_2022$WHIP)
predict_starters <- data.frame(startersW, startersL, startersIP, startersH, startersHR, startersSO, startersBAOpp, startersWHIP)
starter_predictions <- predict(startersFinal, newdata = predict_starters)
starter_predictions <- as.data.frame(starter_predictions)
starter_predictions <- mutate(starter_predictions, predicted_votes = exp(starter_predictions))
starter_predictions <- mutate(starter_predictions, last_name = stats_2022$last_name)
starter_predictions <- mutate(starter_predictions, first_name = stats_2022$first_name)
starter_predictions <- mutate(starter_predictions, LG = stats_2022$LG)
colnames(starter_predictions)[1] <- 'log_prediction'
```

Lets import data on qualified relievers from 2022.

```{r}
relieverstats <- read_csv("relieverstats - Copy.csv")
```

Run our relievers predictive model and make predictions.

```{r}
relieverW <- c(relieverstats$W)
relieverL <- c(relieverstats$L)
relieverIP <- c(relieverstats$IP)
relieverHR <- c(relieverstats$HR)
relieverBB <- c(relieverstats$BB)
relieverSO <- c(relieverstats$SO)
relieverERA <- c(relieverstats$ERA)
relieverSV <- c(relieverstats$SV)
predict_relievers <- data.frame(relieverW, relieverL, relieverIP, relieverHR, relieverBB, relieverSO, relieverERA, relieverSV)
reliever_predictions <- predict(relieverFinal, newdata = predict_relievers)
reliever_predictions <- as.data.frame(reliever_predictions)
reliever_predictions <- mutate(reliever_predictions, predicted_votes = exp(reliever_predictions))
reliever_predictions <- mutate(reliever_predictions, last_name = relieverstats$last_name)
reliever_predictions <- mutate(reliever_predictions, first_name = relieverstats$first_name)
reliever_predictions <- mutate(reliever_predictions, LG = relieverstats$LG)
colnames(reliever_predictions)[1] <- 'log_prediction'
```

Merge starter and reliever predictions into one final 2022 Cy Young Predictions data frame and show the top 20 results.

```{r}
predictions<-rbind(starter_predictions,reliever_predictions)
predictions<-predictions[order(-predictions$predicted_votes),]
head(predictions, n = 20)
```









